![](https://github.com/Shilpaj1994/ML-Assignments/blob/master/Assignment%201/banner.png?raw=true)



---

### Shilpaj Bhalerao

### Batch No 8



# Forward and Back Propagation

![](https://github.com/Shilpaj1994/ML-Assignments/blob/master/Assignment%201/network.png?raw=true)

- Above image shows a small neural network with one input layer, output layer and a hidden layer.

- Below shown pictures are the calculations done by the network during each epoch

- If whole of the dataset is passed in a single batch then:

  forward_propagation + back_propagation = 1 epoch

- Step 0 - 4 are the calculations for forward propagation
- Step 5 - 11 are the calculations for back propagation
- Following calculations also include calculations for biases which are no more used



## Forward Propagation:

#### Step 0: Read Input and Output

- `x` is an input matrix and `y` is a output associated with the input matrix
- So our aim is to get output which is equal to the `y` or close to `y`

![Step 0](https://github.com/Shilpaj1994/ML-Assignments/blob/master/Assignment%201/0.jpg?raw=true)



#### Step 1: Initialize weights and biases with random values

- Initialize the weights and biases of input layer and hidden layer for this network
- The weights and biases are randomly initialized for the network

![Step 1](https://github.com/Shilpaj1994/ML-Assignments/blob/master/Assignment%201/1.JPG?raw=true)



#### Step 2: Calculate Hidden Layer Input

- `hidden_layer_input = matrix_dot_product(X,wh) + bh `
- Using above formula, hidden_layer_inputs are calculated
- This is known as linear transformation

![Step 2](https://github.com/Shilpaj1994/ML-Assignments/blob/master/Assignment%201/2.JPG?raw=true)



#### Step 3: Perform non-linear transformation on hidden linear input

- `hiddenlayer_activations = sigmoid(hidden_layer_input)`
- Sigmoid function:  sigmoid(x) = 1 / (1 + exp(-x))
- Substituting each element of matrix `hidden_layer_input` in sigmoid function, we get corresponding element of `hidden_layer_activation` matrix

![Step 3](https://github.com/Shilpaj1994/ML-Assignments/blob/master/Assignment%201/3.JPG?raw=true)



#### Step 4: Perform linear and non-linear transformation of hidden layer activation at output layer

```python
output_layer_input = matrix_dot_product (hiddenlayer_activations * wout ) + bout
output = sigmoid(output_layer_input)
```

- Using above equations,  `output_layer_input` and `output` is calculated
- Steps from step 0 - 4 is known as forward propagation
- If the whole dataset is passed through network in one batch, forward propagation is done only once
- If there are `m` number of mini-batches then forward propagation is done `m` number of times

![Step 4](https://github.com/Shilpaj1994/ML-Assignments/blob/master/Assignment%201/4.JPG?raw=true)

---

## Back-propagation

#### Step 5: Calculate gradient of Error(E) at output layer

- Gradient of error is calculated using predicted output and actual output
- `E = y-output`

![Step 5](https://github.com/Shilpaj1994/ML-Assignments/blob/master/Assignment%201/5.JPG?raw=true)



#### Step 6: Compute slope at output and hidden layer

- Derivative of sigmoid is given by `x * (1 - x)`
-  Compute the slope/ gradient of hidden and output layer neurons
- To compute the slope, we calculate the derivatives of non-linear activations `x` at each layer for each neuron

```
Slope_output_layer= derivatives_sigmoid(output)
Slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)
```

![Step 6](https://github.com/Shilpaj1994/ML-Assignments/blob/master/Assignment%201/6.JPG?raw=true)



#### Step 7: Compute delta at output layer

- Change factor(delta) at output layer is calculated by multiplying gradient of error with the slope of output layer activation
- ` d_output = E * slope_output_layer * lr`

![Step 7](https://github.com/Shilpaj1994/ML-Assignments/blob/master/Assignment%201/7.JPG?raw=true)



#### Step 8: Calculate Error at hidden layer

- Now, the error will propagate back in the network i.e. to the hidden layer
- The dot-product of output layer delta with weight parameters gives the error at hidden layer
- `Error_at_hidden_layer = matrix_dot_product(d_output, wout.Transpose)`

![Step 8](https://github.com/Shilpaj1994/ML-Assignments/blob/master/Assignment%201/8.JPG?raw=true)



#### Step 9: Compute delta at hidden layer

- Calculate change factor delta at hidden layer
- It is given by product of error at hidden layer and slope of hidden layer activation
- `d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer`

![Step 9](https://github.com/Shilpaj1994/ML-Assignments/blob/master/Assignment%201/9.JPG?raw=true)



#### Step 10: Update weight at both output and hidden layer

- The randomly initialized weights are updated from errors calculated for training sample
- Following are the equations for updating weights 

```
wout= wout+matrix_dot_product(hiddenlayer_activations.Transpose,d_output)*learning_rate
wh =  wh+ matrix_dot_product(X.Transpose,d_hiddenlayer)*learning_rate
```

![Step 10](https://github.com/Shilpaj1994/ML-Assignments/blob/master/Assignment%201/10.JPG?raw=true)



#### Step 11: Update biases at both output and hidden layer

- Biases are updated from the aggregated error at the neuron
- Following are the equations for updating biases

```
bh = bh + sum(d_hiddenlayer, axis=0) * learning_rate
bout = bout + sum(d_output, axis=0)*learning_rate
```

![Step 11](https://github.com/Shilpaj1994/ML-Assignments/blob/master/Assignment%201/11.JPG?raw=true)



---

