{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "K70hAckqg0EA",
    "outputId": "2fc6e1dd-b97c-46c9-9b74-1738093a5255"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "notebook 5.4.0 requires ipykernel, which is not installed.\n",
      "jupyter 1.0.0 requires ipykernel, which is not installed.\n",
      "jupyter-console 5.2.0 requires ipykernel, which is not installed.\n",
      "ipywidgets 7.1.1 requires ipykernel>=4.5.1, which is not installed.\n",
      "You are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# https://keras.io/\n",
    "!pip install -q keras\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wVIx_KIigxPV"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Concatenate\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UNHw6luQg3gc"
   },
   "outputs": [],
   "source": [
    "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
    "# backend\n",
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# Create a session with the above options specified.\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dsO_yGxcg5D8"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 256\n",
    "num_classes = 10\n",
    "epochs = 250\n",
    "l = 40\n",
    "num_filter = 12\n",
    "compression = 0.5\n",
    "dropout_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "mB7o3zu1g6eT",
    "outputId": "c1cea922-a38d-45da-f9d8-7977ab9c2dd2"
   },
   "outputs": [],
   "source": [
    "# Load CIFAR10 Data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
    "\n",
    "# convert to one hot encoing \n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ee-sge5Kg7vr"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "def add_denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(l):\n",
    "        BatchNorm = BatchNormalization()(temp)\n",
    "        relu = Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
    "        if dropout_rate>0:\n",
    "          Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n",
    "        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        \n",
    "        temp = concat\n",
    "        \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OOP6IPsGhBwb"
   },
   "outputs": [],
   "source": [
    "def add_transition(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    BatchNorm = BatchNormalization()(input)\n",
    "    relu = Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
    "    if dropout_rate>0:\n",
    "      Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
    "    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0RaKFpubhDIC"
   },
   "outputs": [],
   "source": [
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = BatchNormalization()(input)\n",
    "    relu = Activation('relu')(BatchNorm)\n",
    "    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    flat = Flatten()(AvgPooling)\n",
    "    output = Dense(num_classes, activation='softmax')(flat)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "anPCpQWhhGb7"
   },
   "outputs": [],
   "source": [
    "num_filter = 12\n",
    "dropout_rate = 0.2\n",
    "l = 12\n",
    "input = Input(shape=(img_height, img_width, channel,))\n",
    "First_Conv2D = Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
    "\n",
    "First_Block = add_denseblock(First_Conv2D, num_filter, dropout_rate)\n",
    "First_Transition = add_transition(First_Block, num_filter, dropout_rate)\n",
    "\n",
    "Second_Block = add_denseblock(First_Transition, num_filter, dropout_rate)\n",
    "Second_Transition = add_transition(Second_Block, num_filter, dropout_rate)\n",
    "\n",
    "Third_Block = add_denseblock(Second_Transition, num_filter, dropout_rate)\n",
    "Third_Transition = add_transition(Third_Block, num_filter, dropout_rate)\n",
    "\n",
    "Last_Block = add_denseblock(Third_Transition,  num_filter, dropout_rate)\n",
    "output = output_layer(Last_Block)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 9860
    },
    "colab_type": "code",
    "id": "1kFh7pdxhNtT",
    "outputId": "160abc05-9e09-4454-d453-0e33a7d95796"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 12)   324         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 12)   48          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 12)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 6)    648         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32, 32, 6)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 18)   0           conv2d_1[0][0]                   \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 18)   72          concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 18)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 6)    972         activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32, 32, 6)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 24)   0           concatenate_1[0][0]              \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 24)   96          concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 24)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 6)    1296        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32, 32, 6)    0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32, 32, 30)   0           concatenate_2[0][0]              \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 30)   120         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 30)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 6)    1620        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32, 32, 6)    0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 32, 32, 36)   0           concatenate_3[0][0]              \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 36)   144         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 36)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 6)    1944        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 32, 32, 6)    0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 32, 32, 42)   0           concatenate_4[0][0]              \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 42)   168         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 42)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 6)    2268        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 32, 32, 6)    0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 32, 32, 48)   0           concatenate_5[0][0]              \n",
      "                                                                 dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 48)   192         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 48)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 6)    2592        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 32, 32, 6)    0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 32, 32, 54)   0           concatenate_6[0][0]              \n",
      "                                                                 dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 54)   216         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 54)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 6)    2916        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 32, 32, 6)    0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 32, 32, 60)   0           concatenate_7[0][0]              \n",
      "                                                                 dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 60)   240         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 60)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 6)    3240        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 32, 32, 6)    0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 32, 32, 66)   0           concatenate_8[0][0]              \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 66)   264         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 66)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 6)    3564        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 32, 32, 6)    0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 32, 32, 72)   0           concatenate_9[0][0]              \n",
      "                                                                 dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 72)   288         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 72)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 6)    3888        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 32, 32, 6)    0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 32, 32, 78)   0           concatenate_10[0][0]             \n",
      "                                                                 dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 78)   312         concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 78)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 6)    4212        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 32, 32, 6)    0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 32, 32, 84)   0           concatenate_11[0][0]             \n",
      "                                                                 dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 84)   336         concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 84)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 32, 6)    504         activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 32, 32, 6)    0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 16, 16, 6)    0           dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 6)    24          average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 6)    0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 6)    324         activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 16, 16, 6)    0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 16, 16, 12)   0           average_pooling2d_1[0][0]        \n",
      "                                                                 dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 12)   48          concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 12)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 16, 6)    648         activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 16, 16, 6)    0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 16, 16, 18)   0           concatenate_13[0][0]             \n",
      "                                                                 dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 18)   72          concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 18)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 16, 6)    972         activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 16, 16, 6)    0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 16, 16, 24)   0           concatenate_14[0][0]             \n",
      "                                                                 dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 16, 24)   96          concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 24)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 16, 6)    1296        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 16, 16, 6)    0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 16, 16, 30)   0           concatenate_15[0][0]             \n",
      "                                                                 dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 16, 30)   120         concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 16, 30)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 16, 6)    1620        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 16, 16, 6)    0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 16, 16, 36)   0           concatenate_16[0][0]             \n",
      "                                                                 dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 16, 16, 36)   144         concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 16, 16, 36)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 6)    1944        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 16, 16, 6)    0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 16, 16, 42)   0           concatenate_17[0][0]             \n",
      "                                                                 dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 16, 16, 42)   168         concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 16, 16, 42)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 16, 16, 6)    2268        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 16, 16, 6)    0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 16, 16, 48)   0           concatenate_18[0][0]             \n",
      "                                                                 dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 16, 16, 48)   192         concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 16, 16, 48)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 16, 16, 6)    2592        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 16, 16, 6)    0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 16, 16, 54)   0           concatenate_19[0][0]             \n",
      "                                                                 dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 16, 16, 54)   216         concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 16, 16, 54)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 16, 16, 6)    2916        activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 16, 16, 6)    0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 16, 16, 60)   0           concatenate_20[0][0]             \n",
      "                                                                 dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 16, 16, 60)   240         concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 16, 16, 60)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 16, 16, 6)    3240        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 16, 16, 6)    0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 16, 16, 66)   0           concatenate_21[0][0]             \n",
      "                                                                 dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 16, 16, 66)   264         concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 16, 16, 66)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 16, 16, 6)    3564        activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 16, 16, 6)    0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 16, 16, 72)   0           concatenate_22[0][0]             \n",
      "                                                                 dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 16, 16, 72)   288         concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16, 16, 72)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 16, 16, 6)    3888        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 16, 16, 6)    0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 16, 16, 78)   0           concatenate_23[0][0]             \n",
      "                                                                 dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 16, 16, 78)   312         concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 16, 16, 78)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 16, 16, 6)    468         activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 16, 16, 6)    0           conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 8, 8, 6)      0           dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 8, 8, 6)      24          average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 8, 8, 6)      0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 8, 8, 6)      324         activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 8, 8, 6)      0           conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 8, 8, 12)     0           average_pooling2d_2[0][0]        \n",
      "                                                                 dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 8, 8, 12)     48          concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 8, 8, 12)     0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 8, 8, 6)      648         activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 8, 8, 6)      0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 8, 8, 18)     0           concatenate_25[0][0]             \n",
      "                                                                 dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 8, 8, 18)     72          concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 8, 8, 18)     0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 8, 8, 6)      972         activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 8, 8, 6)      0           conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 8, 8, 24)     0           concatenate_26[0][0]             \n",
      "                                                                 dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 8, 8, 24)     96          concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 8, 8, 24)     0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 8, 8, 6)      1296        activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 8, 8, 6)      0           conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 8, 8, 30)     0           concatenate_27[0][0]             \n",
      "                                                                 dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 8, 8, 30)     120         concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 8, 8, 30)     0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 8, 8, 6)      1620        activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 8, 8, 6)      0           conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 8, 8, 36)     0           concatenate_28[0][0]             \n",
      "                                                                 dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 8, 8, 36)     144         concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 8, 8, 36)     0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 8, 8, 6)      1944        activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 8, 8, 6)      0           conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 8, 8, 42)     0           concatenate_29[0][0]             \n",
      "                                                                 dropout_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 8, 8, 42)     168         concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 8, 8, 42)     0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 8, 8, 6)      2268        activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, 8, 8, 6)      0           conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 8, 8, 48)     0           concatenate_30[0][0]             \n",
      "                                                                 dropout_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 8, 8, 48)     192         concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 8, 8, 48)     0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 8, 8, 6)      2592        activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 8, 8, 6)      0           conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)    (None, 8, 8, 54)     0           concatenate_31[0][0]             \n",
      "                                                                 dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 8, 8, 54)     216         concatenate_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 8, 8, 54)     0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 8, 8, 6)      2916        activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 8, 8, 6)      0           conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, 8, 8, 60)     0           concatenate_32[0][0]             \n",
      "                                                                 dropout_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 8, 8, 60)     240         concatenate_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 8, 8, 60)     0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 8, 8, 6)      3240        activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)            (None, 8, 8, 6)      0           conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, 8, 8, 66)     0           concatenate_33[0][0]             \n",
      "                                                                 dropout_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 8, 8, 66)     264         concatenate_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 8, 8, 66)     0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 8, 8, 6)      3564        activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 8, 8, 6)      0           conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 8, 8, 72)     0           concatenate_34[0][0]             \n",
      "                                                                 dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 8, 8, 72)     288         concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 8, 8, 72)     0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 8, 8, 6)      3888        activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 8, 8, 6)      0           conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_36 (Concatenate)    (None, 8, 8, 78)     0           concatenate_35[0][0]             \n",
      "                                                                 dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 8, 8, 78)     312         concatenate_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 8, 8, 78)     0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 8, 8, 6)      468         activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 8, 8, 6)      0           conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 4, 4, 6)      0           dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 4, 4, 6)      24          average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 4, 4, 6)      0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 4, 4, 6)      324         activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 4, 4, 6)      0           conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_37 (Concatenate)    (None, 4, 4, 12)     0           average_pooling2d_3[0][0]        \n",
      "                                                                 dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 4, 4, 12)     48          concatenate_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 4, 4, 12)     0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 4, 4, 6)      648         activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 4, 4, 6)      0           conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_38 (Concatenate)    (None, 4, 4, 18)     0           concatenate_37[0][0]             \n",
      "                                                                 dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 4, 4, 18)     72          concatenate_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 4, 4, 18)     0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 4, 4, 6)      972         activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 4, 4, 6)      0           conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_39 (Concatenate)    (None, 4, 4, 24)     0           concatenate_38[0][0]             \n",
      "                                                                 dropout_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 4, 4, 24)     96          concatenate_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 4, 4, 24)     0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 4, 4, 6)      1296        activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 4, 4, 6)      0           conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_40 (Concatenate)    (None, 4, 4, 30)     0           concatenate_39[0][0]             \n",
      "                                                                 dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 4, 4, 30)     120         concatenate_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 4, 4, 30)     0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 4, 4, 6)      1620        activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, 4, 4, 6)      0           conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_41 (Concatenate)    (None, 4, 4, 36)     0           concatenate_40[0][0]             \n",
      "                                                                 dropout_44[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 4, 4, 36)     144         concatenate_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 4, 4, 36)     0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 4, 4, 6)      1944        activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)            (None, 4, 4, 6)      0           conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_42 (Concatenate)    (None, 4, 4, 42)     0           concatenate_41[0][0]             \n",
      "                                                                 dropout_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 4, 4, 42)     168         concatenate_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 4, 4, 42)     0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 4, 4, 6)      2268        activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_46 (Dropout)            (None, 4, 4, 6)      0           conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_43 (Concatenate)    (None, 4, 4, 48)     0           concatenate_42[0][0]             \n",
      "                                                                 dropout_46[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 4, 4, 48)     192         concatenate_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 4, 4, 48)     0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 4, 4, 6)      2592        activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)            (None, 4, 4, 6)      0           conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_44 (Concatenate)    (None, 4, 4, 54)     0           concatenate_43[0][0]             \n",
      "                                                                 dropout_47[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 4, 4, 54)     216         concatenate_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 4, 4, 54)     0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 4, 4, 6)      2916        activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)            (None, 4, 4, 6)      0           conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_45 (Concatenate)    (None, 4, 4, 60)     0           concatenate_44[0][0]             \n",
      "                                                                 dropout_48[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 4, 4, 60)     240         concatenate_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 4, 4, 60)     0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 4, 4, 6)      3240        activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 4, 4, 6)      0           conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_46 (Concatenate)    (None, 4, 4, 66)     0           concatenate_45[0][0]             \n",
      "                                                                 dropout_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 4, 4, 66)     264         concatenate_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 4, 4, 66)     0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 4, 4, 6)      3564        activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)            (None, 4, 4, 6)      0           conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_47 (Concatenate)    (None, 4, 4, 72)     0           concatenate_46[0][0]             \n",
      "                                                                 dropout_50[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 4, 4, 72)     288         concatenate_47[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 4, 4, 72)     0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 4, 4, 6)      3888        activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_51 (Dropout)            (None, 4, 4, 6)      0           conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_48 (Concatenate)    (None, 4, 4, 78)     0           concatenate_47[0][0]             \n",
      "                                                                 dropout_51[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 4, 4, 78)     312         concatenate_48[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 4, 4, 78)     0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 2, 2, 78)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 312)          0           average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           3130        flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 118,918\n",
      "Trainable params: 114,394\n",
      "Non-trainable params: 4,524\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4XOsW3ahSkL"
   },
   "outputs": [],
   "source": [
    "# determine Loss function and Optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1771
    },
    "colab_type": "code",
    "id": "crhGk7kEhXAz",
    "outputId": "e3e2d0d0-1492-41ab-df5b-5a7ecd705c2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/250\n",
      "50000/50000 [==============================] - 77s 2ms/step - loss: 2.3196 - acc: 0.1296 - val_loss: 2.2837 - val_acc: 0.1265\n",
      "Epoch 2/250\n",
      "50000/50000 [==============================] - 64s 1ms/step - loss: 2.1877 - acc: 0.1934 - val_loss: 2.2525 - val_acc: 0.1504\n",
      "Epoch 3/250\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 2.0054 - acc: 0.2536 - val_loss: 2.0258 - val_acc: 0.2349\n",
      "Epoch 4/250\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 1.8366 - acc: 0.3005 - val_loss: 1.8662 - val_acc: 0.2882\n",
      "Epoch 5/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.7507 - acc: 0.3275 - val_loss: 1.8544 - val_acc: 0.3268\n",
      "Epoch 6/250\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 1.7015 - acc: 0.3425 - val_loss: 1.7477 - val_acc: 0.3476\n",
      "Epoch 7/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.6637 - acc: 0.3611 - val_loss: 2.1598 - val_acc: 0.2871\n",
      "Epoch 8/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.6290 - acc: 0.3767 - val_loss: 1.6831 - val_acc: 0.3854\n",
      "Epoch 9/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.5942 - acc: 0.3900 - val_loss: 1.8293 - val_acc: 0.3478\n",
      "Epoch 10/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.5591 - acc: 0.4069 - val_loss: 1.8903 - val_acc: 0.3543\n",
      "Epoch 11/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 1.5331 - acc: 0.4207 - val_loss: 1.9634 - val_acc: 0.3432\n",
      "Epoch 12/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 1.5095 - acc: 0.4339 - val_loss: 1.6985 - val_acc: 0.4029\n",
      "Epoch 13/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.4877 - acc: 0.4423 - val_loss: 1.5596 - val_acc: 0.4419\n",
      "Epoch 14/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.4658 - acc: 0.4523 - val_loss: 1.6341 - val_acc: 0.4217\n",
      "Epoch 15/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.4466 - acc: 0.4633 - val_loss: 1.5077 - val_acc: 0.4621\n",
      "Epoch 16/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.4324 - acc: 0.4665 - val_loss: 1.8068 - val_acc: 0.3769\n",
      "Epoch 17/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.4122 - acc: 0.4757 - val_loss: 1.6166 - val_acc: 0.4406\n",
      "Epoch 18/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.4006 - acc: 0.4820 - val_loss: 1.5241 - val_acc: 0.4613\n",
      "Epoch 19/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.3803 - acc: 0.4902 - val_loss: 1.5155 - val_acc: 0.4603\n",
      "Epoch 20/250\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 1.3690 - acc: 0.4948 - val_loss: 2.3072 - val_acc: 0.3417\n",
      "Epoch 21/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 1.3525 - acc: 0.5026 - val_loss: 1.5332 - val_acc: 0.4566\n",
      "Epoch 22/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.3384 - acc: 0.5066 - val_loss: 1.3640 - val_acc: 0.5048\n",
      "Epoch 23/250\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 1.3250 - acc: 0.5131 - val_loss: 2.0196 - val_acc: 0.3902\n",
      "Epoch 24/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.3145 - acc: 0.5153 - val_loss: 1.6591 - val_acc: 0.4690\n",
      "Epoch 25/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.3019 - acc: 0.5226 - val_loss: 1.4687 - val_acc: 0.4887\n",
      "Epoch 26/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.2920 - acc: 0.5261 - val_loss: 1.5593 - val_acc: 0.4731\n",
      "Epoch 27/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 1.2790 - acc: 0.5330 - val_loss: 1.3613 - val_acc: 0.5223\n",
      "Epoch 28/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 1.2681 - acc: 0.5350 - val_loss: 1.3937 - val_acc: 0.5152\n",
      "Epoch 29/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 1.2577 - acc: 0.5399 - val_loss: 1.4909 - val_acc: 0.4907\n",
      "Epoch 30/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.2522 - acc: 0.5424 - val_loss: 1.4450 - val_acc: 0.5066\n",
      "Epoch 31/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.2411 - acc: 0.5461 - val_loss: 1.2635 - val_acc: 0.5530\n",
      "Epoch 32/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.2310 - acc: 0.5488 - val_loss: 2.0442 - val_acc: 0.3993\n",
      "Epoch 33/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.2210 - acc: 0.5538 - val_loss: 1.3849 - val_acc: 0.5261\n",
      "Epoch 34/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.2125 - acc: 0.5592 - val_loss: 1.6181 - val_acc: 0.4909\n",
      "Epoch 35/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.2061 - acc: 0.5607 - val_loss: 1.2487 - val_acc: 0.5627\n",
      "Epoch 36/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.1998 - acc: 0.5628 - val_loss: 1.4232 - val_acc: 0.5253\n",
      "Epoch 37/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.1936 - acc: 0.5643 - val_loss: 1.3942 - val_acc: 0.5321\n",
      "Epoch 38/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.1828 - acc: 0.5698 - val_loss: 1.5776 - val_acc: 0.4894\n",
      "Epoch 39/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.1787 - acc: 0.5725 - val_loss: 1.2123 - val_acc: 0.5786\n",
      "Epoch 40/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.1722 - acc: 0.5718 - val_loss: 1.2536 - val_acc: 0.5542\n",
      "Epoch 41/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.1615 - acc: 0.5774 - val_loss: 1.2835 - val_acc: 0.5467\n",
      "Epoch 42/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.1568 - acc: 0.5797 - val_loss: 1.2145 - val_acc: 0.5793\n",
      "Epoch 43/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.1513 - acc: 0.5795 - val_loss: 1.2175 - val_acc: 0.5798\n",
      "Epoch 44/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.1445 - acc: 0.5823 - val_loss: 1.1849 - val_acc: 0.5824\n",
      "Epoch 45/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 1.1354 - acc: 0.5891 - val_loss: 1.6011 - val_acc: 0.4983\n",
      "Epoch 46/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 1.1315 - acc: 0.5864 - val_loss: 1.3726 - val_acc: 0.5424\n",
      "Epoch 47/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 1.1273 - acc: 0.5905 - val_loss: 1.7561 - val_acc: 0.4808\n",
      "Epoch 48/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.1221 - acc: 0.5908 - val_loss: 1.1756 - val_acc: 0.5874\n",
      "Epoch 49/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 1.1114 - acc: 0.5971 - val_loss: 1.1791 - val_acc: 0.5909\n",
      "Epoch 50/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.1070 - acc: 0.5958 - val_loss: 1.2848 - val_acc: 0.5582\n",
      "Epoch 51/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.1040 - acc: 0.5978 - val_loss: 1.1967 - val_acc: 0.5822\n",
      "Epoch 52/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.0974 - acc: 0.6017 - val_loss: 1.1486 - val_acc: 0.6018\n",
      "Epoch 53/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.0929 - acc: 0.6017 - val_loss: 1.2565 - val_acc: 0.5786\n",
      "Epoch 54/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.0851 - acc: 0.6066 - val_loss: 1.2142 - val_acc: 0.5756\n",
      "Epoch 55/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.0814 - acc: 0.6063 - val_loss: 1.7481 - val_acc: 0.4661\n",
      "Epoch 56/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.0791 - acc: 0.6093 - val_loss: 1.1961 - val_acc: 0.5871\n",
      "Epoch 57/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.0759 - acc: 0.6081 - val_loss: 1.3099 - val_acc: 0.5553\n",
      "Epoch 58/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.0655 - acc: 0.6138 - val_loss: 1.2377 - val_acc: 0.5813\n",
      "Epoch 59/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 65s 1ms/step - loss: 1.0637 - acc: 0.6137 - val_loss: 1.3769 - val_acc: 0.5554\n",
      "Epoch 60/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.0580 - acc: 0.6152 - val_loss: 1.3360 - val_acc: 0.5570\n",
      "Epoch 61/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.0526 - acc: 0.6185 - val_loss: 1.4206 - val_acc: 0.5267\n",
      "Epoch 62/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.0465 - acc: 0.6212 - val_loss: 1.1907 - val_acc: 0.5887\n",
      "Epoch 63/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.0439 - acc: 0.6211 - val_loss: 1.4075 - val_acc: 0.5476\n",
      "Epoch 64/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.0403 - acc: 0.6239 - val_loss: 1.6197 - val_acc: 0.4981\n",
      "Epoch 65/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 1.0313 - acc: 0.6260 - val_loss: 1.1206 - val_acc: 0.6082\n",
      "Epoch 66/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 1.0332 - acc: 0.6238 - val_loss: 1.2872 - val_acc: 0.5817\n",
      "Epoch 67/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 1.0274 - acc: 0.6254 - val_loss: 1.1012 - val_acc: 0.6119\n",
      "Epoch 68/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 1.0242 - acc: 0.6293 - val_loss: 1.3155 - val_acc: 0.5703\n",
      "Epoch 69/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 1.0172 - acc: 0.6304 - val_loss: 1.3617 - val_acc: 0.5613\n",
      "Epoch 70/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 1.0155 - acc: 0.6323 - val_loss: 1.1277 - val_acc: 0.6136\n",
      "Epoch 71/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 1.0116 - acc: 0.6345 - val_loss: 1.1310 - val_acc: 0.6074\n",
      "Epoch 72/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1.0063 - acc: 0.6366 - val_loss: 1.0961 - val_acc: 0.6192\n",
      "Epoch 73/250\n",
      "50000/50000 [==============================] - 68s 1ms/step - loss: 1.0067 - acc: 0.6345 - val_loss: 1.0744 - val_acc: 0.6295\n",
      "Epoch 74/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 1.0003 - acc: 0.6395 - val_loss: 1.1385 - val_acc: 0.6155\n",
      "Epoch 75/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.9939 - acc: 0.6383 - val_loss: 1.9085 - val_acc: 0.4720\n",
      "Epoch 76/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.9903 - acc: 0.6392 - val_loss: 1.1508 - val_acc: 0.6076\n",
      "Epoch 77/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.9883 - acc: 0.6405 - val_loss: 1.1955 - val_acc: 0.6029\n",
      "Epoch 78/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.9850 - acc: 0.6432 - val_loss: 1.1800 - val_acc: 0.6059\n",
      "Epoch 79/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.9826 - acc: 0.6449 - val_loss: 1.3336 - val_acc: 0.5788\n",
      "Epoch 80/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.9786 - acc: 0.6457 - val_loss: 1.1548 - val_acc: 0.6168\n",
      "Epoch 81/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.9738 - acc: 0.6482 - val_loss: 1.3625 - val_acc: 0.5618\n",
      "Epoch 82/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.9727 - acc: 0.6479 - val_loss: 1.3754 - val_acc: 0.5661\n",
      "Epoch 83/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.9678 - acc: 0.6513 - val_loss: 1.2408 - val_acc: 0.5836\n",
      "Epoch 84/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.9657 - acc: 0.6520 - val_loss: 1.0843 - val_acc: 0.6342\n",
      "Epoch 85/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.9617 - acc: 0.6531 - val_loss: 0.9915 - val_acc: 0.6583\n",
      "Epoch 86/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.9621 - acc: 0.6544 - val_loss: 1.2750 - val_acc: 0.5965\n",
      "Epoch 87/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.9562 - acc: 0.6545 - val_loss: 1.2872 - val_acc: 0.5861\n",
      "Epoch 88/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.9540 - acc: 0.6559 - val_loss: 1.0995 - val_acc: 0.6227\n",
      "Epoch 89/250\n",
      "50000/50000 [==============================] - 68s 1ms/step - loss: 0.9505 - acc: 0.6572 - val_loss: 1.3708 - val_acc: 0.5604\n",
      "Epoch 90/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.9467 - acc: 0.6572 - val_loss: 1.1207 - val_acc: 0.6224\n",
      "Epoch 91/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.9475 - acc: 0.6568 - val_loss: 1.0536 - val_acc: 0.6464\n",
      "Epoch 92/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.9433 - acc: 0.6570 - val_loss: 1.1238 - val_acc: 0.6337\n",
      "Epoch 93/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.9412 - acc: 0.6614 - val_loss: 1.0585 - val_acc: 0.6436\n",
      "Epoch 94/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.9361 - acc: 0.6613 - val_loss: 1.6042 - val_acc: 0.5224\n",
      "Epoch 95/250\n",
      "50000/50000 [==============================] - 68s 1ms/step - loss: 0.9330 - acc: 0.6629 - val_loss: 1.0911 - val_acc: 0.6299\n",
      "Epoch 96/250\n",
      "50000/50000 [==============================] - 68s 1ms/step - loss: 0.9314 - acc: 0.6611 - val_loss: 1.0546 - val_acc: 0.6470\n",
      "Epoch 97/250\n",
      "50000/50000 [==============================] - 68s 1ms/step - loss: 0.9278 - acc: 0.6650 - val_loss: 1.0920 - val_acc: 0.6344\n",
      "Epoch 98/250\n",
      "50000/50000 [==============================] - 68s 1ms/step - loss: 0.9269 - acc: 0.6657 - val_loss: 1.1185 - val_acc: 0.6244\n",
      "Epoch 99/250\n",
      "50000/50000 [==============================] - 68s 1ms/step - loss: 0.9195 - acc: 0.6667 - val_loss: 1.3124 - val_acc: 0.5818\n",
      "Epoch 100/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.9165 - acc: 0.6688 - val_loss: 1.5400 - val_acc: 0.5388\n",
      "Epoch 101/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.9173 - acc: 0.6674 - val_loss: 1.0442 - val_acc: 0.6330\n",
      "Epoch 102/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.9166 - acc: 0.6684 - val_loss: 0.9640 - val_acc: 0.6632\n",
      "Epoch 103/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.9120 - acc: 0.6689 - val_loss: 1.1792 - val_acc: 0.6321\n",
      "Epoch 104/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.9112 - acc: 0.6707 - val_loss: 1.0356 - val_acc: 0.6483\n",
      "Epoch 105/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.9076 - acc: 0.6736 - val_loss: 1.0495 - val_acc: 0.6496\n",
      "Epoch 106/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.9035 - acc: 0.6750 - val_loss: 1.0354 - val_acc: 0.6510\n",
      "Epoch 107/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8999 - acc: 0.6736 - val_loss: 1.0424 - val_acc: 0.6407\n",
      "Epoch 108/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.9006 - acc: 0.6740 - val_loss: 0.9686 - val_acc: 0.6701\n",
      "Epoch 109/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8990 - acc: 0.6751 - val_loss: 1.0912 - val_acc: 0.6347\n",
      "Epoch 110/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8936 - acc: 0.6769 - val_loss: 1.0116 - val_acc: 0.6583\n",
      "Epoch 111/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8951 - acc: 0.6778 - val_loss: 1.0669 - val_acc: 0.6463\n",
      "Epoch 112/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8897 - acc: 0.6775 - val_loss: 1.0319 - val_acc: 0.6525\n",
      "Epoch 113/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8847 - acc: 0.6811 - val_loss: 1.2064 - val_acc: 0.6110\n",
      "Epoch 114/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.8852 - acc: 0.6805 - val_loss: 1.0898 - val_acc: 0.6334\n",
      "Epoch 115/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8874 - acc: 0.6786 - val_loss: 1.0067 - val_acc: 0.6576\n",
      "Epoch 116/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8843 - acc: 0.6821 - val_loss: 1.1581 - val_acc: 0.6206\n",
      "Epoch 117/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8795 - acc: 0.6827 - val_loss: 1.0156 - val_acc: 0.6600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8767 - acc: 0.6857 - val_loss: 1.0554 - val_acc: 0.6496\n",
      "Epoch 119/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8771 - acc: 0.6848 - val_loss: 1.2925 - val_acc: 0.5856\n",
      "Epoch 120/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8751 - acc: 0.6832 - val_loss: 0.9731 - val_acc: 0.6680\n",
      "Epoch 121/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8714 - acc: 0.6867 - val_loss: 1.0115 - val_acc: 0.6564\n",
      "Epoch 122/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8703 - acc: 0.6858 - val_loss: 0.9859 - val_acc: 0.6670\n",
      "Epoch 123/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8669 - acc: 0.6854 - val_loss: 1.0470 - val_acc: 0.6496\n",
      "Epoch 124/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8621 - acc: 0.6893 - val_loss: 1.0451 - val_acc: 0.6531\n",
      "Epoch 125/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8633 - acc: 0.6884 - val_loss: 1.2580 - val_acc: 0.6057\n",
      "Epoch 126/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8647 - acc: 0.6884 - val_loss: 0.9532 - val_acc: 0.6793\n",
      "Epoch 127/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8611 - acc: 0.6890 - val_loss: 0.9544 - val_acc: 0.6751\n",
      "Epoch 128/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.8556 - acc: 0.6909 - val_loss: 0.9747 - val_acc: 0.6735\n",
      "Epoch 129/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8496 - acc: 0.6929 - val_loss: 1.0393 - val_acc: 0.6613\n",
      "Epoch 130/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8481 - acc: 0.6958 - val_loss: 1.0735 - val_acc: 0.6408\n",
      "Epoch 131/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8479 - acc: 0.6956 - val_loss: 0.9951 - val_acc: 0.6632\n",
      "Epoch 132/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8477 - acc: 0.6948 - val_loss: 1.1110 - val_acc: 0.6357\n",
      "Epoch 133/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8473 - acc: 0.6963 - val_loss: 1.0675 - val_acc: 0.6554\n",
      "Epoch 134/250\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 0.8434 - acc: 0.6961 - val_loss: 1.1239 - val_acc: 0.6377\n",
      "Epoch 135/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8400 - acc: 0.6970 - val_loss: 1.1630 - val_acc: 0.6187\n",
      "Epoch 136/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8390 - acc: 0.6986 - val_loss: 1.0806 - val_acc: 0.6542\n",
      "Epoch 137/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.8383 - acc: 0.6999 - val_loss: 1.3170 - val_acc: 0.6116\n",
      "Epoch 138/250\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 0.8317 - acc: 0.7012 - val_loss: 0.9526 - val_acc: 0.6740\n",
      "Epoch 139/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8344 - acc: 0.7008 - val_loss: 1.4859 - val_acc: 0.5721\n",
      "Epoch 140/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8336 - acc: 0.6990 - val_loss: 0.9727 - val_acc: 0.6823\n",
      "Epoch 141/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8282 - acc: 0.7016 - val_loss: 1.1509 - val_acc: 0.6333\n",
      "Epoch 142/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8293 - acc: 0.6998 - val_loss: 1.0149 - val_acc: 0.6616\n",
      "Epoch 143/250\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 0.8275 - acc: 0.7029 - val_loss: 1.4481 - val_acc: 0.5665\n",
      "Epoch 144/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8262 - acc: 0.7037 - val_loss: 0.9526 - val_acc: 0.6781\n",
      "Epoch 145/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8228 - acc: 0.7054 - val_loss: 0.9881 - val_acc: 0.6803\n",
      "Epoch 146/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8257 - acc: 0.7031 - val_loss: 0.9591 - val_acc: 0.6766\n",
      "Epoch 147/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8171 - acc: 0.7078 - val_loss: 0.9538 - val_acc: 0.6821\n",
      "Epoch 148/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8158 - acc: 0.7055 - val_loss: 1.0443 - val_acc: 0.6577\n",
      "Epoch 149/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8180 - acc: 0.7064 - val_loss: 1.0337 - val_acc: 0.6604\n",
      "Epoch 150/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8177 - acc: 0.7080 - val_loss: 0.9799 - val_acc: 0.6839\n",
      "Epoch 151/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8116 - acc: 0.7082 - val_loss: 1.0902 - val_acc: 0.6620\n",
      "Epoch 152/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8132 - acc: 0.7079 - val_loss: 0.9123 - val_acc: 0.6977\n",
      "Epoch 153/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8062 - acc: 0.7124 - val_loss: 1.0050 - val_acc: 0.6777\n",
      "Epoch 154/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8107 - acc: 0.7093 - val_loss: 0.9004 - val_acc: 0.6951\n",
      "Epoch 155/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8019 - acc: 0.7120 - val_loss: 0.9602 - val_acc: 0.6891\n",
      "Epoch 156/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.8044 - acc: 0.7113 - val_loss: 0.9409 - val_acc: 0.6905\n",
      "Epoch 157/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8012 - acc: 0.7124 - val_loss: 1.0166 - val_acc: 0.6705\n",
      "Epoch 158/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7989 - acc: 0.7129 - val_loss: 1.0721 - val_acc: 0.6671\n",
      "Epoch 159/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7980 - acc: 0.7131 - val_loss: 0.9855 - val_acc: 0.6811\n",
      "Epoch 160/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.8000 - acc: 0.7136 - val_loss: 1.2832 - val_acc: 0.6157\n",
      "Epoch 161/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7974 - acc: 0.7130 - val_loss: 0.9518 - val_acc: 0.6855\n",
      "Epoch 162/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7963 - acc: 0.7149 - val_loss: 1.0956 - val_acc: 0.6460\n",
      "Epoch 163/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7922 - acc: 0.7157 - val_loss: 0.9640 - val_acc: 0.6864\n",
      "Epoch 164/250\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 0.7901 - acc: 0.7153 - val_loss: 1.0582 - val_acc: 0.6737\n",
      "Epoch 165/250\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 0.7907 - acc: 0.7163 - val_loss: 1.2009 - val_acc: 0.6267\n",
      "Epoch 166/250\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 0.7908 - acc: 0.7151 - val_loss: 0.9956 - val_acc: 0.6905\n",
      "Epoch 167/250\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 0.7850 - acc: 0.7169 - val_loss: 1.2475 - val_acc: 0.6320\n",
      "Epoch 168/250\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 0.7847 - acc: 0.7186 - val_loss: 0.9316 - val_acc: 0.6928\n",
      "Epoch 169/250\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 0.7830 - acc: 0.7185 - val_loss: 0.9433 - val_acc: 0.6855\n",
      "Epoch 170/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7822 - acc: 0.7206 - val_loss: 1.0809 - val_acc: 0.6629\n",
      "Epoch 171/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7823 - acc: 0.7191 - val_loss: 0.9452 - val_acc: 0.6997\n",
      "Epoch 172/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7824 - acc: 0.7195 - val_loss: 0.9797 - val_acc: 0.6860\n",
      "Epoch 173/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7731 - acc: 0.7232 - val_loss: 1.0235 - val_acc: 0.6736\n",
      "Epoch 174/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7764 - acc: 0.7244 - val_loss: 0.9989 - val_acc: 0.6718\n",
      "Epoch 175/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7726 - acc: 0.7223 - val_loss: 0.8790 - val_acc: 0.7072\n",
      "Epoch 176/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7747 - acc: 0.7219 - val_loss: 0.8713 - val_acc: 0.7105\n",
      "Epoch 177/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7697 - acc: 0.7229 - val_loss: 1.0784 - val_acc: 0.6701\n",
      "Epoch 178/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7683 - acc: 0.7257 - val_loss: 0.8747 - val_acc: 0.7016\n",
      "Epoch 179/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7674 - acc: 0.7244 - val_loss: 0.9162 - val_acc: 0.6960\n",
      "Epoch 180/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7665 - acc: 0.7247 - val_loss: 0.8983 - val_acc: 0.6987\n",
      "Epoch 181/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7645 - acc: 0.7261 - val_loss: 0.8916 - val_acc: 0.7015\n",
      "Epoch 182/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7655 - acc: 0.7245 - val_loss: 1.0548 - val_acc: 0.6705\n",
      "Epoch 183/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7618 - acc: 0.7279 - val_loss: 0.9964 - val_acc: 0.6886\n",
      "Epoch 184/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7573 - acc: 0.7292 - val_loss: 1.0448 - val_acc: 0.6750\n",
      "Epoch 185/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7601 - acc: 0.7270 - val_loss: 0.8665 - val_acc: 0.7126\n",
      "Epoch 186/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7591 - acc: 0.7279 - val_loss: 1.0429 - val_acc: 0.6694\n",
      "Epoch 187/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7580 - acc: 0.7289 - val_loss: 0.9341 - val_acc: 0.7014\n",
      "Epoch 188/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7559 - acc: 0.7301 - val_loss: 0.9974 - val_acc: 0.6713\n",
      "Epoch 189/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7517 - acc: 0.7294 - val_loss: 1.0106 - val_acc: 0.6765\n",
      "Epoch 190/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7510 - acc: 0.7324 - val_loss: 1.1243 - val_acc: 0.6600\n",
      "Epoch 191/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7495 - acc: 0.7324 - val_loss: 1.1643 - val_acc: 0.6504\n",
      "Epoch 192/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7542 - acc: 0.7296 - val_loss: 0.9841 - val_acc: 0.6842\n",
      "Epoch 193/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7511 - acc: 0.7295 - val_loss: 1.2302 - val_acc: 0.6268\n",
      "Epoch 194/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7441 - acc: 0.7358 - val_loss: 0.9309 - val_acc: 0.7026\n",
      "Epoch 195/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7452 - acc: 0.7334 - val_loss: 1.1189 - val_acc: 0.6630\n",
      "Epoch 196/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7437 - acc: 0.7300 - val_loss: 1.0397 - val_acc: 0.6730\n",
      "Epoch 197/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7408 - acc: 0.7327 - val_loss: 0.9634 - val_acc: 0.6901\n",
      "Epoch 198/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7391 - acc: 0.7368 - val_loss: 0.8975 - val_acc: 0.7131\n",
      "Epoch 199/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7420 - acc: 0.7342 - val_loss: 1.0100 - val_acc: 0.6841\n",
      "Epoch 200/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7406 - acc: 0.7349 - val_loss: 0.8504 - val_acc: 0.7231\n",
      "Epoch 201/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7374 - acc: 0.7366 - val_loss: 0.9885 - val_acc: 0.6907\n",
      "Epoch 202/250\n",
      "50000/50000 [==============================] - 68s 1ms/step - loss: 0.7362 - acc: 0.7371 - val_loss: 0.8983 - val_acc: 0.7110\n",
      "Epoch 203/250\n",
      "50000/50000 [==============================] - 68s 1ms/step - loss: 0.7351 - acc: 0.7382 - val_loss: 0.9423 - val_acc: 0.6927\n",
      "Epoch 204/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7307 - acc: 0.7394 - val_loss: 0.8162 - val_acc: 0.7264\n",
      "Epoch 205/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7317 - acc: 0.7397 - val_loss: 1.0293 - val_acc: 0.6759\n",
      "Epoch 206/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7368 - acc: 0.7352 - val_loss: 0.8624 - val_acc: 0.7187\n",
      "Epoch 207/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7295 - acc: 0.7408 - val_loss: 0.9174 - val_acc: 0.7048\n",
      "Epoch 208/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7307 - acc: 0.7391 - val_loss: 0.9721 - val_acc: 0.6939\n",
      "Epoch 209/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7272 - acc: 0.7405 - val_loss: 0.9000 - val_acc: 0.7097\n",
      "Epoch 210/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7309 - acc: 0.7372 - val_loss: 0.9391 - val_acc: 0.7060\n",
      "Epoch 211/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7252 - acc: 0.7401 - val_loss: 0.9189 - val_acc: 0.7029\n",
      "Epoch 212/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7271 - acc: 0.7401 - val_loss: 0.9783 - val_acc: 0.6944\n",
      "Epoch 213/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7224 - acc: 0.7427 - val_loss: 1.1235 - val_acc: 0.6529\n",
      "Epoch 214/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7223 - acc: 0.7426 - val_loss: 1.2298 - val_acc: 0.6333\n",
      "Epoch 215/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7233 - acc: 0.7400 - val_loss: 1.1618 - val_acc: 0.6542\n",
      "Epoch 216/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7215 - acc: 0.7428 - val_loss: 1.0242 - val_acc: 0.6717\n",
      "Epoch 217/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7219 - acc: 0.7418 - val_loss: 0.8814 - val_acc: 0.7143\n",
      "Epoch 218/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7203 - acc: 0.7428 - val_loss: 0.8553 - val_acc: 0.7234\n",
      "Epoch 219/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7174 - acc: 0.7438 - val_loss: 0.8429 - val_acc: 0.7201\n",
      "Epoch 220/250\n",
      "50000/50000 [==============================] - 68s 1ms/step - loss: 0.7128 - acc: 0.7431 - val_loss: 0.8723 - val_acc: 0.7067\n",
      "Epoch 221/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7157 - acc: 0.7437 - val_loss: 0.9358 - val_acc: 0.7048\n",
      "Epoch 222/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7153 - acc: 0.7445 - val_loss: 0.8203 - val_acc: 0.7235\n",
      "Epoch 223/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7118 - acc: 0.7449 - val_loss: 0.8746 - val_acc: 0.7203\n",
      "Epoch 224/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7127 - acc: 0.7450 - val_loss: 1.0668 - val_acc: 0.6615\n",
      "Epoch 225/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7103 - acc: 0.7464 - val_loss: 0.8505 - val_acc: 0.7195\n",
      "Epoch 226/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7073 - acc: 0.7486 - val_loss: 0.9455 - val_acc: 0.7051\n",
      "Epoch 227/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7102 - acc: 0.7451 - val_loss: 0.8431 - val_acc: 0.7292\n",
      "Epoch 228/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7021 - acc: 0.7504 - val_loss: 1.0625 - val_acc: 0.6827\n",
      "Epoch 229/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7112 - acc: 0.7467 - val_loss: 0.9575 - val_acc: 0.6988\n",
      "Epoch 230/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7045 - acc: 0.7487 - val_loss: 0.8785 - val_acc: 0.7173\n",
      "Epoch 231/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7072 - acc: 0.7476 - val_loss: 0.8904 - val_acc: 0.7103\n",
      "Epoch 232/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7049 - acc: 0.7488 - val_loss: 0.8319 - val_acc: 0.7243\n",
      "Epoch 233/250\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7011 - acc: 0.7494 - val_loss: 0.8153 - val_acc: 0.7318\n",
      "Epoch 234/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 67s 1ms/step - loss: 0.7018 - acc: 0.7502 - val_loss: 0.7997 - val_acc: 0.7344\n",
      "Epoch 235/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7015 - acc: 0.7496 - val_loss: 1.0420 - val_acc: 0.6720\n",
      "Epoch 236/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.7004 - acc: 0.7509 - val_loss: 0.9247 - val_acc: 0.7110\n",
      "Epoch 237/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.6949 - acc: 0.7524 - val_loss: 0.8387 - val_acc: 0.7221\n",
      "Epoch 238/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.6919 - acc: 0.7547 - val_loss: 0.8980 - val_acc: 0.7179\n",
      "Epoch 239/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.6932 - acc: 0.7538 - val_loss: 0.9565 - val_acc: 0.7037\n",
      "Epoch 240/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.6936 - acc: 0.7521 - val_loss: 0.8959 - val_acc: 0.7157\n",
      "Epoch 241/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.6924 - acc: 0.7543 - val_loss: 0.9646 - val_acc: 0.7005\n",
      "Epoch 242/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.6899 - acc: 0.7544 - val_loss: 1.0020 - val_acc: 0.6976\n",
      "Epoch 243/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.6941 - acc: 0.7533 - val_loss: 1.0205 - val_acc: 0.6816\n",
      "Epoch 244/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.6889 - acc: 0.7547 - val_loss: 0.8955 - val_acc: 0.7083\n",
      "Epoch 245/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.6934 - acc: 0.7517 - val_loss: 0.9387 - val_acc: 0.6974\n",
      "Epoch 246/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.6888 - acc: 0.7542 - val_loss: 0.9654 - val_acc: 0.7059\n",
      "Epoch 247/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.6853 - acc: 0.7567 - val_loss: 0.8403 - val_acc: 0.7269\n",
      "Epoch 248/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.6871 - acc: 0.7533 - val_loss: 0.8405 - val_acc: 0.7195\n",
      "Epoch 249/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.6867 - acc: 0.7543 - val_loss: 0.9760 - val_acc: 0.7053\n",
      "Epoch 250/250\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 0.6835 - acc: 0.7568 - val_loss: 0.9453 - val_acc: 0.7034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16282645a90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ZcWydmIVhZGr",
    "outputId": "a0345aa5-79ff-4e56-eb94-50437b43c4fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 739us/step\n",
      "Test loss: 0.9452508332252503\n",
      "Test accuracy: 0.7034\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UE3lF6EH1r_L",
    "outputId": "92df862c-76a7-4a02-9533-6c164bc5984d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Save the trained weights in to .h5 format\n",
    "model.save_weights(\"DNST_modelv2.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ai-yZ2ED5AK1"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "\n",
    "# files.download('DNST_model.h5')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "DNST_CIFAR10_AUG.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
